{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark and RDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siu1997/Big-Data-Analytics/blob/main/Practical%203/PySpark_and_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncyi8B64AkBw"
      },
      "source": [
        "## **PRACTICAL 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-zvr6VlAzB_"
      },
      "source": [
        "### **AIM:**\n",
        "To implement PySpark and RDD using python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLoSEsbYA5du"
      },
      "source": [
        "### **THEORY:**\n",
        "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n",
        "Apache Spark is a lightning fast real-time processing framework. It does in-memory computations to analyze data in real-time. It came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. Hence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.\n",
        "\n",
        "Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms also. Apache Spark has its own cluster manager, where it can host its application. It leverages Apache Hadoop for both storage and processing. It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaB_bi8CCT9S"
      },
      "source": [
        "Install PySpark and related dependencies in Google Collab. Also Create and test a RDD.\n",
        "\n",
        "Install Java, PySpark and FindSpark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fEXxSVlCRlA"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cXzgqanO7mg"
      },
      "source": [
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL9-lkPoO_ID"
      },
      "source": [
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDHqCGoYCejQ"
      },
      "source": [
        "Set environment variables \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBVoFEiICfXT"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-sO3rkqC1eI"
      },
      "source": [
        "Check for executable files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kouj1bkIC2To",
        "outputId": "0fc01738-fc27-4c85-d273-4475356e33e9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\t   spark-3.1.1-bin-hadoop3.2.tgz\n",
            "spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ6KqUTaC52e"
      },
      "source": [
        "Initialize findSpark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYIgXGACC6zg"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axWg-7MAC_2w"
      },
      "source": [
        "Create RDD named sc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPlgITdFDA7A"
      },
      "source": [
        "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) \n",
        "# Property used to format output tables better\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ50nNidQY6o",
        "outputId": "d0f61684-9052-4dfc-d6b3-870e83e5c1d7"
      },
      "source": [
        "#Creating spark context-Its like connecting to spark cluster\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        " \n",
        "sc1 = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "print(sc1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5GRkzKYQuje"
      },
      "source": [
        "\n",
        "Create a Spark API to check the total number of Prime Numbers in the given range from 0 to n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwfQR9SjQ1iW"
      },
      "source": [
        "1.Define a function to check whether the number is prime or not?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSX-1oFcQr2V"
      },
      "source": [
        "def isprime(n):\n",
        "    \"\"\"\n",
        "    check if integer n is a prime\n",
        "    \"\"\"\n",
        "    # make sure n is a positive integer\n",
        "    n = abs(int(n))\n",
        "    # 0 and 1 are not primes\n",
        "    if n < 2:\n",
        "        return False\n",
        "    # 2 is the only even prime number\n",
        "    if n == 2:\n",
        "        return True\n",
        "    # all other even numbers are not primes\n",
        "    if not n & 1:\n",
        "        return False\n",
        "    # range starts with 3 and only needs to go up the square root of n\n",
        "    # for all odd numbers\n",
        "    for x in range(3, int(n**0.5)+1, 2):\n",
        "        if n % x == 0:\n",
        "            return False\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtt5gFzIQ8pl"
      },
      "source": [
        "2. Create a Spark RDD and parallelize the operation of checking the prime numbers using parallelize() operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqP-5hPSQ9rD"
      },
      "source": [
        "# Create an RDD of numbers from 0 to 1,000,000\n",
        "nums = sc1.parallelize(range(1000000))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPPh8LZRKcs"
      },
      "source": [
        "3. Get the total number of prime numbers using filter() operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPfcl115RLkL",
        "outputId": "fed2d649-988e-4c9a-806c-11b52fe46593"
      },
      "source": [
        "# Compute the number of primes in the RDD\n",
        "print(nums.filter(isprime).count())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72M7HH9MRWoU"
      },
      "source": [
        "\n",
        " Write a Word Count API using PySpark and RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJWIDmNSRbR1"
      },
      "source": [
        "1.Create a Spark RDD. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njD-Y_-0RYub"
      },
      "source": [
        "#Creating spark context-Its like connecting to spark cluster\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGcbsDdyRl9H"
      },
      "source": [
        "2. Load the contents of the ex.txt file into ‘text’ RDD.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eud6i18Rogl",
        "outputId": "9e94658c-79d0-4d77-b60b-04e8993a8ef9"
      },
      "source": [
        "text = sc.textFile(\"ex.txt\")\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ex.txt MapPartitionsRDD[18] at textFile at NativeMethodAccessorImpl.java:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flt2NWS9RtHN"
      },
      "source": [
        "3. Import add "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQi-egz3RuLh"
      },
      "source": [
        "from operator import add\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd2mjuNvR0Qy"
      },
      "source": [
        "4.Define a function to split each line of text using  (Space)” “ seperator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g05GE6wHRzkU"
      },
      "source": [
        "\n",
        "def tokenize(text):\n",
        "     return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQNp1dlaR4mv"
      },
      "source": [
        "5. Tokenize each sentence into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qI7u_A2R7T_",
        "outputId": "4383ac6c-9cda-4b8e-f6f7-87872df6d796"
      },
      "source": [
        "words = text.flatMap(tokenize)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PythonRDD[19] at RDD at PythonRDD.scala:53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc4k8S6QSCsh"
      },
      "source": [
        "6. Map each word to a tuple (word, 1), where one represents the  occurrence of a word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKMuGSVeSANU",
        "outputId": "1a09e8c6-b22d-4418-88aa-4d350f087adb"
      },
      "source": [
        "\n",
        "wc = words.map(lambda x: (x,1))\n",
        "wc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[20] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ffta26oSF0B"
      },
      "source": [
        "7. Display the List of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRDeX0_DSAHd",
        "outputId": "4ad0df17-8972-41be-d3b3-28cca00f8080"
      },
      "source": [
        "print(wc.toDebugString())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'(2) PythonRDD[20] at RDD at PythonRDD.scala:53 []\\n |  ex.txt MapPartitionsRDD[18] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  ex.txt HadoopRDD[17] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNdlKGYSJCB"
      },
      "source": [
        "8. Reduce all the words based on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ytw1_WSObx",
        "outputId": "d73d2751-019f-4ea3-d8bf-532ce44e2511"
      },
      "source": [
        "counts = wc.reduceByKey(add)\n",
        "counts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[25] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwM7ZfhnSQyX"
      },
      "source": [
        "9. Save word counts into a text file in ‘ec’ folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kRF9fQcSTO3"
      },
      "source": [
        "counts.saveAsTextFile(\"wc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36w2Ls4XSVLR"
      },
      "source": [
        "10. Check the contents of wc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15vsqK9kSXbR",
        "outputId": "ababbb8d-be8c-45a8-8531-466bbecf1561"
      },
      "source": [
        "!ls wc/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000  part-00001\t_SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cCBO8PPTDAv"
      },
      "source": [
        "11. Display the List of words and their frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfGNtRl_TEES",
        "outputId": "01ae14da-bc89-4c3d-b607-9d9d49e757bf"
      },
      "source": [
        "!head wc/part-00000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('eius', 7)\n",
            "('modi.', 2)\n",
            "('porro', 4)\n",
            "('consectetur', 7)\n",
            "('dolorem.', 2)\n",
            "('ipsum', 6)\n",
            "('amet', 5)\n",
            "('porro.', 1)\n",
            "('Magnam', 3)\n",
            "('quaerat', 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3K9ToqlQvX0"
      },
      "source": [
        " Creating PySpark RDD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPQJXc_1QxWS"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwLpTOBwQ7uO"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5Q-eCORAER"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "450vo-e_REhp"
      },
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.appName('SparkByExamples').getOrCreate()\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jRpHEy9RJcc"
      },
      "source": [
        "Create Spark RDD Using Parallelized Collections\n",
        " \n",
        "SparkContext has several functions to use with RDDs. For example, it’s parallelize() method is used to create an RDD from a list.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaEBfTMQRKXJ",
        "outputId": "85696de3-3ba7-4153-adf4-37dd8e358cd2"
      },
      "source": [
        "#Create RDD from parallelize    \n",
        "dataList = [(\"Hadoop\", 10), (\"MapReduce\", 20), (\"Spark\", 30)]\n",
        "rdd=sc.sparkContext.parallelize(dataList)\n",
        "rdd\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[29] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pQ7q4vfRQzp",
        "outputId": "a398b1f5-29d1-400c-fb2f-e81323aa3ebe"
      },
      "source": [
        "words = sc.sparkContext.parallelize (\n",
        " [\"scala\",\n",
        " \"java\",\n",
        " \"hadoop\",\n",
        " \"spark\",\n",
        " \"akka\",\n",
        " \"spark vs hadoop\",\n",
        " \"pyspark\",\n",
        " \"pyspark and spark\"]\n",
        ")\n",
        "words\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[30] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujdXkNKARhh7",
        "outputId": "e6580442-0035-467a-e9eb-cab9e3a1bb78"
      },
      "source": [
        "#Example 3: Create empty RDD using sparkContext.emptyRDD\n",
        "# Creates empty RDD with no partition    \n",
        "rdd = sc.sparkContext.emptyRDD \n",
        "# rddString = spark.sparkContext.emptyRDD[String]\n",
        "print(rdd)\n",
        "#Example 4: Create empty RDD with Partition\n",
        "#Create empty RDD with partition\n",
        "rdd1 = sc.sparkContext.parallelize([],5) #This creates 5 partitions\n",
        "print(rdd1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method SparkContext.emptyRDD of <SparkContext master=local[*] appName=pyspark-shell>>\n",
            "ParallelCollectionRDD[31] at readRDDFromFile at PythonRDD.scala:274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFC58V4kTQ8X"
      },
      "source": [
        " \n",
        "# Create Spark RDD External Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea9UfZ9CTP4Y"
      },
      "source": [
        "#Create RDD from external Data source\n",
        "rdd2 = sc.sparkContext.textFile(\"ex.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Lb9vjiU662",
        "outputId": "ef691d35-dc90-41f8-e506-f3910221c457"
      },
      "source": [
        "words=sc.sparkContext.parallelize(['one','two''three','four','five'])\n",
        " \n",
        "w=words.filter(lambda x:'o' in x)\n",
        "print(w) \n",
        "f=w.collect()\n",
        "print(f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PythonRDD[35] at RDD at PythonRDD.scala:53\n",
            "['one', 'twothree', 'four']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}